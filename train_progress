Model 1

Epoch 1

loss: 4.682214, 32.000000/50000.000000
loss: 4.006253, 352.000000/50000.000000
loss: 4.004165, 672.000000/50000.000000
loss: 4.008071, 992.000000/50000.000000
loss: 4.025258, 1312.000000/50000.000000
loss: 4.061196, 1632.000000/50000.000000
loss: 3.997914, 1952.000000/50000.000000
loss: 3.987758, 2272.000000/50000.000000
loss: 4.038539, 2592.000000/50000.000000
loss: 4.038539, 2912.000000/50000.000000
loss: 4.050258, 3232.000000/50000.000000
loss: 4.046352, 3552.000000/50000.000000
loss: 4.076039, 3872.000000/50000.000000
loss: 4.000258, 4192.000000/50000.000000
loss: 4.022133, 4512.000000/50000.000000
loss: 4.019008, 4832.000000/50000.000000
loss: 4.047133, 5152.000000/50000.000000
loss: 4.018227, 5472.000000/50000.000000
loss: 3.974477, 5792.000000/50000.000000
loss: 3.976039, 6112.000000/50000.000000
loss: 4.059633, 6432.000000/50000.000000
loss: 3.997133, 6752.000000/50000.000000
loss: 3.996352, 7072.000000/50000.000000
loss: 3.990883, 7392.000000/50000.000000
loss: 4.010415, 7712.000000/50000.000000
loss: 4.016665, 8032.000000/50000.000000
loss: 4.069789, 8352.000000/50000.000000
loss: 4.022133, 8672.000000/50000.000000
loss: 3.997133, 8992.000000/50000.000000
loss: 4.029164, 9312.000000/50000.000000
loss: 4.059633, 9632.000000/50000.000000
loss: 4.011977, 9952.000000/50000.000000
loss: 4.038539, 10272.000000/50000.000000
loss: 4.008852, 10592.000000/50000.000000
loss: 4.020571, 10912.000000/50000.000000
loss: 4.051821, 11232.000000/50000.000000
loss: 4.036196, 11552.000000/50000.000000
loss: 4.056508, 11872.000000/50000.000000
loss: 4.004946, 12192.000000/50000.000000
loss: 4.017446, 12512.000000/50000.000000
loss: 3.986196, 12832.000000/50000.000000
loss: 4.044789, 13152.000000/50000.000000
loss: 4.057290, 13472.000000/50000.000000
loss: 4.030727, 13792.000000/50000.000000
loss: 4.013539, 14112.000000/50000.000000
loss: 4.023696, 14432.000000/50000.000000
loss: 3.984633, 14752.000000/50000.000000
loss: 4.042446, 15072.000000/50000.000000
loss: 4.045571, 15392.000000/50000.000000
loss: 4.033852, 15712.000000/50000.000000


Model 2

Epoch 1

loss: 4.681979, 32.000000/59000.000000
loss: 4.685414, 672.000000/59000.000000
^CTraceback (most recent call last):
  File "/home/harry/Programs/ml/learning.py", line 193, in <module>
    train(train_dataloader, model, loss_function, optimizer)
  File "/home/harry/Programs/ml/learning.py", line 155, in train
    pred = model(image)
           ^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/harry/Programs/ml/learning.py", line 138, in forward
    features = self.conv(image)
               ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/harry/Programs/ml/learning.py", line 41, in forward
    image = self.pool(image) # 300x200
            ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/pooling.py", line 213, in forward
    return F.max_pool2d(
           ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/_jit_internal.py", line 624, in fn
    return if_false(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/functional.py", line 830, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt


ml on  master [!?] via 󰌠 v3.12.7 took 1m53s 
130 by 2 INT ❯ python learning.py
Epoch 1

loss: 4.682302, 32.000000/59000.000000
^CTraceback (most recent call last):
  File "/home/harry/Programs/ml/learning.py", line 202, in <module>
    train(train_dataloader, model, loss_function, optimizer)
  File "/home/harry/Programs/ml/learning.py", line 163, in train
    pred = model(image)
           ^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/harry/Programs/ml/learning.py", line 138, in forward
    features = self.conv(image)
               ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/harry/Programs/ml/learning.py", line 52, in forward
    image = self.conv4(image)
            ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
KeyboardInterrupt
^C

ml on  master [!?] via 󰌠 v3.12.7 took 47s 
130 by 2 INT ❯ python learning.py
Epoch 1

loss: 4.681759, 32.000000/59000.000000
loss: 4.671009, 672.000000/59000.000000
loss: 4.672753, 1312.000000/59000.000000
loss: 4.672378, 1952.000000/59000.000000
loss: 4.671399, 2592.000000/59000.000000
loss: 4.671950, 3232.000000/59000.000000
loss: 4.671620, 3872.000000/59000.000000
loss: 4.672243, 4512.000000/59000.000000
loss: 4.671754, 5152.000000/59000.000000
loss: 4.672292, 5792.000000/59000.000000
loss: 4.672182, 6432.000000/59000.000000
loss: 4.671962, 7072.000000/59000.000000
loss: 4.672036, 7712.000000/59000.000000
loss: 4.671926, 8352.000000/59000.000000
loss: 4.672121, 8992.000000/59000.000000
loss: 4.672365, 9632.000000/59000.000000
loss: 4.672133, 10272.000000/59000.000000
loss: 4.672427, 10912.000000/59000.000000
loss: 4.671888, 11552.000000/59000.000000
loss: 4.671766, 12192.000000/59000.000000
loss: 4.671950, 12832.000000/59000.000000
loss: 4.672011, 13472.000000/59000.000000
loss: 4.671362, 14112.000000/59000.000000
loss: 4.671962, 14752.000000/59000.000000
loss: 4.672158, 15392.000000/59000.000000
loss: 4.671680, 16032.000000/59000.000000
loss: 4.672305, 16672.000000/59000.000000
loss: 4.671632, 17312.000000/59000.000000
loss: 4.671840, 17952.000000/59000.000000
loss: 4.671313, 18592.000000/59000.000000
loss: 4.671717, 19232.000000/59000.000000
loss: 4.672072, 19872.000000/59000.000000
loss: 4.672072, 20512.000000/59000.000000
loss: 4.672452, 21152.000000/59000.000000
loss: 4.671926, 21792.000000/59000.000000
loss: 4.671999, 22432.000000/59000.000000
loss: 4.672084, 23072.000000/59000.000000
loss: 4.672133, 23712.000000/59000.000000
loss: 4.671424, 24352.000000/59000.000000
loss: 4.671705, 24992.000000/59000.000000
loss: 4.672341, 25632.000000/59000.000000
loss: 4.672721, 26272.000000/59000.000000
loss: 4.671962, 26912.000000/59000.000000
loss: 4.672329, 27552.000000/59000.000000
loss: 4.672060, 28192.000000/59000.000000
loss: 4.671754, 28832.000000/59000.000000
loss: 4.672451, 29472.000000/59000.000000
loss: 4.671766, 30112.000000/59000.000000
loss: 4.671595, 30752.000000/59000.000000
loss: 4.671595, 31392.000000/59000.000000
loss: 4.671949, 32032.000000/59000.000000
loss: 4.672721, 32672.000000/59000.000000
loss: 4.672770, 33312.000000/59000.000000
loss: 4.672146, 33952.000000/59000.000000
loss: 4.671913, 34592.000000/59000.000000
loss: 4.671717, 35232.000000/59000.000000
loss: 4.672158, 35872.000000/59000.000000
loss: 4.671522, 36512.000000/59000.000000
loss: 4.671999, 37152.000000/59000.000000
loss: 4.672317, 37792.000000/59000.000000
loss: 4.671950, 38432.000000/59000.000000
loss: 4.672206, 39072.000000/59000.000000
loss: 4.671546, 39712.000000/59000.000000
loss: 4.671534, 40352.000000/59000.000000
loss: 4.672060, 40992.000000/59000.000000
loss: 4.671534, 41632.000000/59000.000000
loss: 4.672256, 42272.000000/59000.000000
loss: 4.671595, 42912.000000/59000.000000
loss: 4.672145, 43552.000000/59000.000000
loss: 4.671888, 44192.000000/59000.000000
loss: 4.672366, 44832.000000/59000.000000
loss: 4.672121, 45472.000000/59000.000000
loss: 4.671889, 46112.000000/59000.000000
loss: 4.671754, 46752.000000/59000.000000
loss: 4.671730, 47392.000000/59000.000000
loss: 4.671901, 48032.000000/59000.000000
loss: 4.672048, 48672.000000/59000.000000
loss: 4.672427, 49312.000000/59000.000000
loss: 4.672598, 49952.000000/59000.000000
loss: 4.671705, 50592.000000/59000.000000
loss: 4.671680, 51232.000000/59000.000000
loss: 4.672170, 51872.000000/59000.000000
loss: 4.672390, 52512.000000/59000.000000
loss: 4.672390, 53152.000000/59000.000000
loss: 4.671926, 53792.000000/59000.000000
loss: 4.671546, 54432.000000/59000.000000
loss: 4.672463, 55072.000000/59000.000000
loss: 4.671852, 55712.000000/59000.000000
loss: 4.672097, 56352.000000/59000.000000
loss: 4.671852, 56992.000000/59000.000000
loss: 4.671949, 57632.000000/59000.000000
loss: 4.671729, 58272.000000/59000.000000
loss: 4.671852, 58912.000000/59000.000000
tensor([[107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107]])
Test Error: 
 Accuracy: 68.2%, Avg loss: 4.671967 

Done!
Epoch 2

loss: 4.672035, 32.000000/59000.000000
loss: 4.672206, 672.000000/59000.000000
loss: 4.671631, 1312.000000/59000.000000
loss: 4.671595, 1952.000000/59000.000000
loss: 4.672183, 2592.000000/59000.000000
loss: 4.671191, 3232.000000/59000.000000
loss: 4.671852, 3872.000000/59000.000000
loss: 4.671803, 4512.000000/59000.000000
loss: 4.672219, 5152.000000/59000.000000
loss: 4.671631, 5792.000000/59000.000000
loss: 4.672622, 6432.000000/59000.000000
loss: 4.671987, 7072.000000/59000.000000
loss: 4.671827, 7712.000000/59000.000000
loss: 4.673002, 8352.000000/59000.000000
loss: 4.670825, 8992.000000/59000.000000
loss: 4.672586, 9632.000000/59000.000000
loss: 4.671179, 10272.000000/59000.000000
loss: 4.671779, 10912.000000/59000.000000
loss: 4.672145, 11552.000000/59000.000000
loss: 4.671448, 12192.000000/59000.000000
loss: 4.672818, 12832.000000/59000.000000
loss: 4.671987, 13472.000000/59000.000000
loss: 4.672048, 14112.000000/59000.000000
loss: 4.672084, 14752.000000/59000.000000
loss: 4.671631, 15392.000000/59000.000000
loss: 4.672096, 16032.000000/59000.000000
loss: 4.672145, 16672.000000/59000.000000
loss: 4.671815, 17312.000000/59000.000000
loss: 4.671852, 17952.000000/59000.000000
loss: 4.672133, 18592.000000/59000.000000
loss: 4.672305, 19232.000000/59000.000000
loss: 4.672684, 19872.000000/59000.000000
loss: 4.671448, 20512.000000/59000.000000
loss: 4.672292, 21152.000000/59000.000000
loss: 4.671485, 21792.000000/59000.000000
loss: 4.672354, 22432.000000/59000.000000
loss: 4.671227, 23072.000000/59000.000000
loss: 4.672268, 23712.000000/59000.000000
loss: 4.672170, 24352.000000/59000.000000
loss: 4.671999, 24992.000000/59000.000000
loss: 4.672023, 25632.000000/59000.000000
loss: 4.671766, 26272.000000/59000.000000
loss: 4.672684, 26912.000000/59000.000000
loss: 4.672623, 27552.000000/59000.000000
loss: 4.671913, 28192.000000/59000.000000
loss: 4.671521, 28832.000000/59000.000000
loss: 4.672439, 29472.000000/59000.000000
loss: 4.671778, 30112.000000/59000.000000
loss: 4.672133, 30752.000000/59000.000000
loss: 4.671852, 31392.000000/59000.000000
loss: 4.671595, 32032.000000/59000.000000
loss: 4.672365, 32672.000000/59000.000000
loss: 4.672536, 33312.000000/59000.000000
loss: 4.671558, 33952.000000/59000.000000
loss: 4.671472, 34592.000000/59000.000000
loss: 4.671558, 35232.000000/59000.000000
loss: 4.672549, 35872.000000/59000.000000
loss: 4.672048, 36512.000000/59000.000000
loss: 4.671595, 37152.000000/59000.000000
loss: 4.672635, 37792.000000/59000.000000
loss: 4.672317, 38432.000000/59000.000000
loss: 4.671876, 39072.000000/59000.000000
loss: 4.671606, 39712.000000/59000.000000
loss: 4.672121, 40352.000000/59000.000000
loss: 4.672084, 40992.000000/59000.000000
loss: 4.672157, 41632.000000/59000.000000
loss: 4.672402, 42272.000000/59000.000000
loss: 4.671814, 42912.000000/59000.000000
loss: 4.671545, 43552.000000/59000.000000
loss: 4.672328, 44192.000000/59000.000000
loss: 4.671655, 44832.000000/59000.000000
loss: 4.671837, 45472.000000/59000.000000
loss: 4.672325, 46112.000000/59000.000000
loss: 4.672366, 46752.000000/59000.000000
loss: 4.672109, 47392.000000/59000.000000
loss: 4.671864, 48032.000000/59000.000000
loss: 4.671448, 48672.000000/59000.000000
loss: 4.671937, 49312.000000/59000.000000
loss: 4.672561, 49952.000000/59000.000000
loss: 4.673063, 50592.000000/59000.000000
loss: 4.672194, 51232.000000/59000.000000
loss: 4.672146, 51872.000000/59000.000000
loss: 4.671680, 52512.000000/59000.000000
loss: 4.672390, 53152.000000/59000.000000
loss: 4.672353, 53792.000000/59000.000000
loss: 4.672096, 54432.000000/59000.000000
loss: 4.672341, 55072.000000/59000.000000
loss: 4.672268, 55712.000000/59000.000000
loss: 4.671313, 56352.000000/59000.000000
loss: 4.672243, 56992.000000/59000.000000
loss: 4.671595, 57632.000000/59000.000000
loss: 4.671595, 58272.000000/59000.000000
loss: 4.671828, 58912.000000/59000.000000
tensor([[107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107]])
Test Error: 
 Accuracy: 68.2%, Avg loss: 4.671967 

Done!
Epoch 3

loss: 4.672023, 32.000000/59000.000000
loss: 4.671583, 672.000000/59000.000000
loss: 4.671779, 1312.000000/59000.000000
loss: 4.672194, 1952.000000/59000.000000
loss: 4.672182, 2592.000000/59000.000000
loss: 4.671668, 3232.000000/59000.000000
loss: 4.672476, 3872.000000/59000.000000
loss: 4.672023, 4512.000000/59000.000000
loss: 4.671289, 5152.000000/59000.000000
loss: 4.672207, 5792.000000/59000.000000
loss: 4.671631, 6432.000000/59000.000000
loss: 4.671949, 7072.000000/59000.000000
loss: 4.671913, 7712.000000/59000.000000
loss: 4.672402, 8352.000000/59000.000000
loss: 4.671632, 8992.000000/59000.000000
loss: 4.672500, 9632.000000/59000.000000
loss: 4.672317, 10272.000000/59000.000000
loss: 4.672366, 10912.000000/59000.000000
loss: 4.672121, 11552.000000/59000.000000
loss: 4.672097, 12192.000000/59000.000000
loss: 4.671362, 12832.000000/59000.000000
loss: 4.672023, 13472.000000/59000.000000
loss: 4.671999, 14112.000000/59000.000000
loss: 4.672084, 14752.000000/59000.000000
loss: 4.671864, 15392.000000/59000.000000
loss: 4.671852, 16032.000000/59000.000000
loss: 4.671730, 16672.000000/59000.000000
loss: 4.672097, 17312.000000/59000.000000
loss: 4.672206, 17952.000000/59000.000000
loss: 4.671815, 18592.000000/59000.000000
loss: 4.672182, 19232.000000/59000.000000
loss: 4.672036, 19872.000000/59000.000000
loss: 4.672194, 20512.000000/59000.000000
loss: 4.672060, 21152.000000/59000.000000
loss: 4.672231, 21792.000000/59000.000000
loss: 4.671668, 22432.000000/59000.000000
loss: 4.671717, 23072.000000/59000.000000
loss: 4.672206, 23712.000000/59000.000000
loss: 4.671412, 24352.000000/59000.000000
loss: 4.671546, 24992.000000/59000.000000
loss: 4.672684, 25632.000000/59000.000000
loss: 4.671705, 26272.000000/59000.000000
loss: 4.672206, 26912.000000/59000.000000
loss: 4.671742, 27552.000000/59000.000000
loss: 4.671827, 28192.000000/59000.000000
loss: 4.671399, 28832.000000/59000.000000
loss: 4.671460, 29472.000000/59000.000000
loss: 4.672244, 30112.000000/59000.000000
loss: 4.671815, 30752.000000/59000.000000
loss: 4.671522, 31392.000000/59000.000000
loss: 4.672524, 32032.000000/59000.000000
loss: 4.672121, 32672.000000/59000.000000
loss: 4.671546, 33312.000000/59000.000000
loss: 4.672561, 33952.000000/59000.000000
loss: 4.671950, 34592.000000/59000.000000
loss: 4.671962, 35232.000000/59000.000000
loss: 4.672121, 35872.000000/59000.000000
loss: 4.672096, 36512.000000/59000.000000
loss: 4.672231, 37152.000000/59000.000000
loss: 4.672402, 37792.000000/59000.000000
loss: 4.672048, 38432.000000/59000.000000
loss: 4.671252, 39072.000000/59000.000000
loss: 4.672488, 39712.000000/59000.000000
loss: 4.672023, 40352.000000/59000.000000
loss: 4.672219, 40992.000000/59000.000000
loss: 4.671656, 41632.000000/59000.000000
loss: 4.671496, 42272.000000/59000.000000
loss: 4.672696, 42912.000000/59000.000000
loss: 4.672892, 43552.000000/59000.000000
loss: 4.672427, 44192.000000/59000.000000
loss: 4.672574, 44832.000000/59000.000000
loss: 4.672072, 45472.000000/59000.000000
loss: 4.672708, 46112.000000/59000.000000
loss: 4.672035, 46752.000000/59000.000000
loss: 4.672158, 47392.000000/59000.000000
loss: 4.672194, 48032.000000/59000.000000
loss: 4.671803, 48672.000000/59000.000000
loss: 4.671815, 49312.000000/59000.000000
loss: 4.671840, 49952.000000/59000.000000
loss: 4.672011, 50592.000000/59000.000000
loss: 4.672476, 51232.000000/59000.000000
loss: 4.671607, 51872.000000/59000.000000
loss: 4.671754, 52512.000000/59000.000000
loss: 4.672280, 53152.000000/59000.000000
loss: 4.672084, 53792.000000/59000.000000
loss: 4.672231, 54432.000000/59000.000000
loss: 4.671277, 55072.000000/59000.000000
loss: 4.671619, 55712.000000/59000.000000
loss: 4.672244, 56352.000000/59000.000000
loss: 4.671718, 56992.000000/59000.000000
loss: 4.671913, 57632.000000/59000.000000
loss: 4.671448, 58272.000000/59000.000000
loss: 4.672183, 58912.000000/59000.000000
tensor([[107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107]])
Test Error: 
 Accuracy: 68.2%, Avg loss: 4.671967 

Done!
Epoch 4

loss: 4.671668, 32.000000/59000.000000
loss: 4.672292, 672.000000/59000.000000
loss: 4.671827, 1312.000000/59000.000000
loss: 4.671949, 1952.000000/59000.000000
loss: 4.671779, 2592.000000/59000.000000
loss: 4.672231, 3232.000000/59000.000000
loss: 4.672256, 3872.000000/59000.000000
loss: 4.671240, 4512.000000/59000.000000
loss: 4.671840, 5152.000000/59000.000000
loss: 4.671937, 5792.000000/59000.000000
loss: 4.671790, 6432.000000/59000.000000
loss: 4.672011, 7072.000000/59000.000000
loss: 4.671779, 7712.000000/59000.000000
loss: 4.672219, 8352.000000/59000.000000
loss: 4.672194, 8992.000000/59000.000000
loss: 4.672268, 9632.000000/59000.000000
loss: 4.671975, 10272.000000/59000.000000
loss: 4.671424, 10912.000000/59000.000000
loss: 4.672097, 11552.000000/59000.000000
loss: 4.671643, 12192.000000/59000.000000
loss: 4.671668, 12832.000000/59000.000000
loss: 4.672684, 13472.000000/59000.000000
loss: 4.672341, 14112.000000/59000.000000
loss: 4.671815, 14752.000000/59000.000000
loss: 4.672219, 15392.000000/59000.000000
loss: 4.670836, 16032.000000/59000.000000
loss: 4.672072, 16672.000000/59000.000000
loss: 4.672231, 17312.000000/59000.000000
loss: 4.671264, 17952.000000/59000.000000
loss: 4.671644, 18592.000000/59000.000000
loss: 4.671987, 19232.000000/59000.000000
loss: 4.672317, 19872.000000/59000.000000
loss: 4.672036, 20512.000000/59000.000000
loss: 4.671619, 21152.000000/59000.000000
loss: 4.672769, 21792.000000/59000.000000
loss: 4.672599, 22432.000000/59000.000000
loss: 4.672720, 23072.000000/59000.000000
loss: 4.671791, 23712.000000/59000.000000
loss: 4.672096, 24352.000000/59000.000000
loss: 4.672647, 24992.000000/59000.000000
loss: 4.672194, 25632.000000/59000.000000
loss: 4.672036, 26272.000000/59000.000000
loss: 4.671987, 26912.000000/59000.000000
loss: 4.671534, 27552.000000/59000.000000
loss: 4.671815, 28192.000000/59000.000000
loss: 4.672170, 28832.000000/59000.000000
loss: 4.672072, 29472.000000/59000.000000
loss: 4.671313, 30112.000000/59000.000000
loss: 4.671154, 30752.000000/59000.000000
loss: 4.672770, 31392.000000/59000.000000
loss: 4.671975, 32032.000000/59000.000000
loss: 4.671803, 32672.000000/59000.000000
loss: 4.672071, 33312.000000/59000.000000
loss: 4.671926, 33952.000000/59000.000000
loss: 4.671852, 34592.000000/59000.000000
loss: 4.671338, 35232.000000/59000.000000
loss: 4.671950, 35872.000000/59000.000000
loss: 4.672561, 36512.000000/59000.000000
loss: 4.672574, 37152.000000/59000.000000
loss: 4.671949, 37792.000000/59000.000000
loss: 4.671766, 38432.000000/59000.000000
loss: 4.672366, 39072.000000/59000.000000
loss: 4.671534, 39712.000000/59000.000000
loss: 4.671925, 40352.000000/59000.000000
loss: 4.673234, 40992.000000/59000.000000
loss: 4.672145, 41632.000000/59000.000000
loss: 4.671986, 42272.000000/59000.000000
loss: 4.672304, 42912.000000/59000.000000
loss: 4.672145, 43552.000000/59000.000000
loss: 4.671631, 44192.000000/59000.000000
loss: 4.672121, 44832.000000/59000.000000
loss: 4.672329, 45472.000000/59000.000000
loss: 4.672060, 46112.000000/59000.000000
loss: 4.671644, 46752.000000/59000.000000
loss: 4.672341, 47392.000000/59000.000000
loss: 4.672390, 48032.000000/59000.000000
loss: 4.671705, 48672.000000/59000.000000
loss: 4.671521, 49312.000000/59000.000000
loss: 4.672010, 49952.000000/59000.000000
loss: 4.672769, 50592.000000/59000.000000
loss: 4.671999, 51232.000000/59000.000000
loss: 4.672060, 51872.000000/59000.000000
loss: 4.672097, 52512.000000/59000.000000
loss: 4.671619, 53152.000000/59000.000000
loss: 4.671888, 53792.000000/59000.000000
loss: 4.673332, 54432.000000/59000.000000
loss: 4.671729, 55072.000000/59000.000000
loss: 4.671509, 55712.000000/59000.000000
loss: 4.672610, 56352.000000/59000.000000
loss: 4.671803, 56992.000000/59000.000000
loss: 4.672475, 57632.000000/59000.000000
loss: 4.672622, 58272.000000/59000.000000
loss: 4.672206, 58912.000000/59000.000000
tensor([[107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107]])
Test Error: 
 Accuracy: 68.2%, Avg loss: 4.671967 

Done!
Epoch 5

loss: 4.672183, 32.000000/59000.000000
loss: 4.672390, 672.000000/59000.000000
loss: 4.671729, 1312.000000/59000.000000
loss: 4.672671, 1952.000000/59000.000000
loss: 4.672635, 2592.000000/59000.000000
loss: 4.671803, 3232.000000/59000.000000
loss: 4.671962, 3872.000000/59000.000000
loss: 4.672060, 4512.000000/59000.000000
loss: 4.671998, 5152.000000/59000.000000
loss: 4.671718, 5792.000000/59000.000000
loss: 4.672280, 6432.000000/59000.000000
loss: 4.671619, 7072.000000/59000.000000
loss: 4.672341, 7712.000000/59000.000000
loss: 4.671595, 8352.000000/59000.000000
loss: 4.671729, 8992.000000/59000.000000
loss: 4.671705, 9632.000000/59000.000000
loss: 4.671790, 10272.000000/59000.000000
loss: 4.672366, 10912.000000/59000.000000
loss: 4.672414, 11552.000000/59000.000000
loss: 4.671277, 12192.000000/59000.000000
loss: 4.672011, 12832.000000/59000.000000
loss: 4.671656, 13472.000000/59000.000000
loss: 4.672280, 14112.000000/59000.000000
loss: 4.671558, 14752.000000/59000.000000
loss: 4.671974, 15392.000000/59000.000000
loss: 4.672145, 16032.000000/59000.000000
loss: 4.672207, 16672.000000/59000.000000
loss: 4.672427, 17312.000000/59000.000000
loss: 4.672023, 17952.000000/59000.000000
loss: 4.672109, 18592.000000/59000.000000
loss: 4.672182, 19232.000000/59000.000000
loss: 4.671619, 19872.000000/59000.000000
loss: 4.672328, 20512.000000/59000.000000
loss: 4.671424, 21152.000000/59000.000000
loss: 4.672353, 21792.000000/59000.000000
loss: 4.672341, 22432.000000/59000.000000
loss: 4.672353, 23072.000000/59000.000000
loss: 4.671852, 23712.000000/59000.000000
loss: 4.671913, 24352.000000/59000.000000
loss: 4.671937, 24992.000000/59000.000000
loss: 4.671888, 25632.000000/59000.000000
loss: 4.672072, 26272.000000/59000.000000
loss: 4.672549, 26912.000000/59000.000000
loss: 4.671766, 27552.000000/59000.000000
loss: 4.671852, 28192.000000/59000.000000
loss: 4.671840, 28832.000000/59000.000000
loss: 4.671704, 29472.000000/59000.000000
loss: 4.672488, 30112.000000/59000.000000
loss: 4.671889, 30752.000000/59000.000000
loss: 4.672586, 31392.000000/59000.000000
loss: 4.672439, 32032.000000/59000.000000
loss: 4.672794, 32672.000000/59000.000000
loss: 4.672782, 33312.000000/59000.000000
loss: 4.671705, 33952.000000/59000.000000
loss: 4.671681, 34592.000000/59000.000000
loss: 4.672096, 35232.000000/59000.000000
loss: 4.671644, 35872.000000/59000.000000
loss: 4.671999, 36512.000000/59000.000000
loss: 4.671815, 37152.000000/59000.000000
loss: 4.671204, 37792.000000/59000.000000
loss: 4.672672, 38432.000000/59000.000000
loss: 4.671815, 39072.000000/59000.000000
loss: 4.671937, 39712.000000/59000.000000
loss: 4.671326, 40352.000000/59000.000000
loss: 4.672365, 40992.000000/59000.000000
loss: 4.672048, 41632.000000/59000.000000
loss: 4.672304, 42272.000000/59000.000000
loss: 4.671705, 42912.000000/59000.000000
loss: 4.672256, 43552.000000/59000.000000
loss: 4.672561, 44192.000000/59000.000000
loss: 4.672280, 44832.000000/59000.000000
loss: 4.672048, 45472.000000/59000.000000
loss: 4.671521, 46112.000000/59000.000000
loss: 4.672097, 46752.000000/59000.000000
loss: 4.671509, 47392.000000/59000.000000
loss: 4.672292, 48032.000000/59000.000000
loss: 4.671889, 48672.000000/59000.000000
loss: 4.671987, 49312.000000/59000.000000
loss: 4.671644, 49952.000000/59000.000000
loss: 4.672207, 50592.000000/59000.000000
loss: 4.671950, 51232.000000/59000.000000
loss: 4.672256, 51872.000000/59000.000000
loss: 4.671534, 52512.000000/59000.000000
loss: 4.672121, 53152.000000/59000.000000
loss: 4.671852, 53792.000000/59000.000000
loss: 4.671975, 54432.000000/59000.000000
loss: 4.671987, 55072.000000/59000.000000
loss: 4.672414, 55712.000000/59000.000000
loss: 4.672158, 56352.000000/59000.000000
loss: 4.672414, 56992.000000/59000.000000
loss: 4.672451, 57632.000000/59000.000000
loss: 4.671656, 58272.000000/59000.000000
loss: 4.671938, 58912.000000/59000.000000
tensor([[107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107],
        [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,
         107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107]])
Test Error: 
 Accuracy: 68.2%, Avg loss: 4.671967 

Done!